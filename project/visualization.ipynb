{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Importing Libraries and Defining Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports essential libraries for the project and sets up configuration paths.\n",
    "\n",
    "### Libraries Imported:\n",
    "- `os`: For interacting with the operating system.\n",
    "- `numpy`: For numerical operations.\n",
    "- `pandas`: For data manipulation.\n",
    "- `rasterio`: For reading and writing geospatial raster data.\n",
    "- `subprocess`: For running subprocesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "import rasterio\n",
    "import subprocess\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import earthpy.plot as ep\n",
    "import earthpy.spatial as es\n",
    "from dataset import read_img, transform_data\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.windows import Window\n",
    "from matplotlib import pyplot as plt\n",
    "from dataset import data_csv_gen, patch_images\n",
    "\n",
    "dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths:\n",
    "- Sets paths for training, validation, and testing datasets.\n",
    "- Sets paths for storing outputs and logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_path) # if image path is empty, it will be error\n",
    "print(mask_path) # if mask path is empty, it will be error\n",
    "\n",
    "if not (os.path.exists(train_dir)):\n",
    "    data_csv_gen() # create csv files if there is no csv\n",
    "\n",
    "# provide csv directory\n",
    "train_df = pd.read_csv(train_dir)\n",
    "test_df = pd.read_csv(test_dir)\n",
    "valid_df = pd.read_csv(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (os.path.exists(p_train_dir)):\n",
    "    # create json file if there is no csv\n",
    "    patch_images(train_df, \"train_phr_cb\") \n",
    "    patch_images(valid_df, \"valid_phr_cb\")\n",
    "    patch_images(test_df, \"test_phr_cb\")\n",
    "\n",
    "# provide json directory\n",
    "p_train_dir = p_train_dir\n",
    "p_valid_dir = p_train_dir\n",
    "p_test_dir = p_test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Counting Images in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell prints the total number of images in the training, testing, and validation datasets.\n",
    "\n",
    "### Outputs:\n",
    "- Total number of training images.\n",
    "- Total number of test images.\n",
    "- Total number of validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Number of images before patchify\")\n",
    "print(f\"Training images = {len(train_df)}\")\n",
    "print(f\"Validation images = {len(valid_df)}\")\n",
    "print(f\"Test images = {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with p_train_dir.open() as j:\n",
    "    train_dir = json.loads(j.read())\n",
    "with p_valid_dir.open() as j:\n",
    "    valid_dir = json.loads(j.read())\n",
    "with p_test_dir.open() as j:\n",
    "    test_dir = json.loads(j.read())\n",
    "            \n",
    "train_features = train_dir[\"feature_ids\"]\n",
    "valid_features = valid_dir[\"feature_ids\"]\n",
    "test_features = valid_dir[\"feature_ids\"]\n",
    "\n",
    "print(\"Total Number of images after patchify\")\n",
    "print(f\"Training images = {len(train_features)}\")\n",
    "print(f\"Validation images = {len(valid_features)}\")\n",
    "print(f\"Test images = {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking Class Balance & Unique Values of Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function to check the class percentage in the full dataset.\n",
    "\n",
    "### Function: `class_balance_check(patchify, data_dir)`\n",
    "- **Parameters**:\n",
    "  - `patchify` (bool): TRUE if class balance is to be checked for patchify experiments.\n",
    "  - `data_dir` (str): Directory where data files are saved.\n",
    "- **Returns**: Class percentage.\n",
    "- **Prints**:\n",
    "  - Class pixel percentage.\n",
    "  - Unique values in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance_check(patchify, data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        Checking class percentage in the full dataset\n",
    "    Arguments:\n",
    "        patchify (bool): TRUE if want to check class balance for patchify experiments\n",
    "        data_dir (str): directory where data files are saved \n",
    "    Return:\n",
    "        Class percentage\n",
    "    \"\"\"\n",
    "    if patchify:\n",
    "        with open(data_dir, \"r\") as j:\n",
    "            train_data = json.loads(j.read())\n",
    "        labels = train_data[\"masks\"]\n",
    "        patch_idx = train_data[\"patch_idx\"]\n",
    "    else:\n",
    "        train_data = data_dir\n",
    "        labels = train_data.masks.values\n",
    "        patch_idx = None\n",
    "\n",
    "    total = 0\n",
    "    class_name = {}\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        with rasterio.open(labels[i]) as msk:\n",
    "            mask = msk.read(1)\n",
    "\n",
    "        if patchify:\n",
    "            idx = patch_idx[i]\n",
    "            mask = mask[idx[0] : idx[1], idx[2] : idx[3]]\n",
    "\n",
    "        total_pix = mask.shape[0] * mask.shape[1]\n",
    "        total += total_pix\n",
    "\n",
    "        dic = {}\n",
    "        keys = np.unique(mask)\n",
    "        for key in keys:\n",
    "            dic[key] = np.count_nonzero(mask == key)\n",
    "\n",
    "        for key, value in dic.items():\n",
    "            if key in class_name.keys():\n",
    "                class_name[key] = value + class_name[key]\n",
    "            else:\n",
    "                class_name[key] = value\n",
    "\n",
    "    for key, val in class_name.items():\n",
    "        class_name[key] = (val / total) * 100\n",
    "\n",
    "    print(\"Class percentage:\")\n",
    "    for key, val in class_name.items():\n",
    "        print(\"class pixel: {} = {}\".format(key, val))\n",
    "    print(f\"Unique value in the mask {class_name.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs the `class_balance_check` function on the dataset.\n",
    "\n",
    "### Outputs:\n",
    "- Class percentage for each class in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Class percentage of training data before patch\")\n",
    "class_balance_check(patchify = False,\n",
    "                    data_dir = train_df)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Class percentage of training data after patch\")\n",
    "class_balance_check(patchify = True,\n",
    "                    data_dir = p_train_dir)\n",
    "print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checking Unique Height and Width of Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function `check_height_width` to check and print unique heights and widths of images and masks in a dataset.\n",
    "\n",
    "### Function: `check_height_width(data_dir)`\n",
    "- **Parameters**: \n",
    "  - `data_dir` (str): Path to the CSV file.\n",
    "- **Process**:\n",
    "  - Reads the CSV file.\n",
    "  - Extracts image and mask paths.\n",
    "  - Iterates through the images and masks to find unique shapes.\n",
    "  - Prints the shapes of the dataset, input images, and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_height_width(data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        check unique hight and width of images from dataset\n",
    "    Arguments:\n",
    "        data_dir (str): path to csv file\n",
    "    Return:\n",
    "        print all the unique height and width\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_dir)\n",
    "\n",
    "    print(\"Dataset:  \", data.shape)\n",
    "\n",
    "    input_img = data.feature_ids.values\n",
    "    input_mask = data.masks.values\n",
    "\n",
    "    input_img_shape = []\n",
    "    input_mask_shape = []\n",
    "\n",
    "    for i in range(len(input_img)):\n",
    "        with rasterio.open(input_img[i]) as im:\n",
    "            img = im.read()\n",
    "        with rasterio.open(input_mask[i]) as msk:\n",
    "            mask = msk.read()\n",
    "        # img = cv2.imread(input_img[i])\n",
    "        # mask = cv2.imread(input_mask[i])\n",
    "        print(f\"Shape for:{i} image Shape:{img.shape}    mask shape:{mask.shape}\")\n",
    "\n",
    "        if img.shape not in input_img_shape:\n",
    "            input_img_shape.append(img.shape)\n",
    "\n",
    "        if mask.shape not in input_mask_shape:\n",
    "            input_mask_shape.append(mask.shape)\n",
    "            \n",
    "    print(\"Input image shapes: \", input_img_shape)\n",
    "    print(\"Input mask shapes: \", input_mask_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Unique height and width of training dataset\")\n",
    "check_height_width(train_dir)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Unique height and width of testing dataset\")\n",
    "check_height_width(test_dir)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Unique height and width of validation dataset\")\n",
    "check_height_width(valid_dir)\n",
    "print(\"----------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting Metrics from CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines functions to handle CSV files and plot metrics against epochs.\n",
    "\n",
    "### Functions:\n",
    "- `return_csv_from_path`: Returns a list of CSV file paths from a directory.\n",
    "- `_plot_from_csv`: Plots specified columns from a CSV file against epochs.\n",
    "- `plot_metrics_vs_epochs`: Plots metrics from a CSV file against epochs using `_plot_from_csv`.\n",
    "- `plot_metric_vs_epochs_vs_models`: Plots a specific metric against epochs for different models and saves the combined results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_csv_from_path(csv_path=csv_logger_path):\n",
    "    csv_list = []\n",
    "    # Iterate through each subdirectory\n",
    "    for folder in csv_path.iterdir():\n",
    "        # Check if the entry is a directory\n",
    "        if folder.is_dir():\n",
    "            # Iterate through files in the subdirectory\n",
    "            for file in folder.iterdir():\n",
    "                # Check if the entry is a file\n",
    "                if file.is_file():\n",
    "                    csv_list.append(file)\n",
    "    return csv_list\n",
    "\n",
    "def _plot_from_csv(csv_path, name, x_axis_name, y_axis_name, columns_to_plot=None):\n",
    "    pathlib.Path((root_dir / \"logs\" / \"plots\" / \"metrics_plots\")).mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    epochs = df['epoch']\n",
    "    if columns_to_plot is not None:\n",
    "        columns_to_plot = columns_to_plot\n",
    "    else:\n",
    "        columns_to_plot = df.columns.to_list()[1:]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for column in columns_to_plot:\n",
    "        plt.plot(epochs, df[column], label=column, linewidth=3.0, marker=\"o\", markersize=5)\n",
    "\n",
    "    plt.title(f\"{y_axis_name}_over_{x_axis_name}\")\n",
    "    plt.xlabel(x_axis_name)\n",
    "    plt.ylabel(y_axis_name)\n",
    "    plt.xticks(epochs.astype(int))\n",
    "    plt.legend()\n",
    "    plt.savefig(root_dir / \"logs\" / \"plots\" / \"metrics_plots\" / name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_vs_epochs(csv_path, name, x_axis_name=\"Epochs\", y_axis_name=\"Metrics_score\", columns_to_plot=None):\n",
    "    _plot_from_csv(csv_path=csv_path, name=name, x_axis_name=x_axis_name, y_axis_name=y_axis_name, columns_to_plot=columns_to_plot)\n",
    "\n",
    "def plot_metric_vs_epochs_vs_models(metric_name=\"my_mean_iou\"):\n",
    "    pathlib.Path((root_dir / \"logs\" / \"plots\" / \"csv_for_plotting\")).mkdir(parents=True, exist_ok=True)\n",
    "    csv_list = return_csv_from_path()\n",
    "    result_df = pd.DataFrame()\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        result_df[os.path.basename(csv_path)] = df[metric_name]\n",
    "    result_df.index.name = \"epoch\"\n",
    "    result_df.to_csv(os.path.join(root_dir / \"logs\" / \"plots\" / \"csv_for_plotting\" / f\"{metric_name}_vs_epoch.csv\"), encoding='utf-8', index=True, header=True)\n",
    "    _plot_from_csv(root_dir / \"logs\" / \"plots\" / \"csv_for_plotting\" / f\"{metric_name}_vs_epoch.csv\", x_axis_name=\"Epochs\", y_axis_name=metric_name, name=metric_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_vs_epochs(csv_logger_path / \"planet-2\" / \"planet-2_ex_2024-07-13_e_4000_p_2048_s_1024_nsr-1_dtype_nsr-1.csv\", name='metrics')\n",
    "plot_metrics_vs_epochs(csv_logger_path / \"planet-2\" / \"planet-2_ex_2024-07-13_e_4000_p_2048_s_1024_nsr-1_dtype_nsr-1.csv\", name='metrics', columns_to_plot=[\"my_mean_iou\"])\n",
    "plot_metric_vs_epochs_vs_models()\n",
    "plot_metric_vs_epochs_vs_models(metric_name=\"my_mean_iou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Displaying and Saving All Images and Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines `pct_clip` & `false_colour` to read an image with dynamic shape and apply percentage clipping to each channel.\n",
    "\n",
    "only for csv files to read and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_clip(array, pct=[2.5, 97.5]):\n",
    "    array_min, array_max = np.nanpercentile(array, pct[0]), np.nanpercentile(array, pct[1])\n",
    "    clip = (array - array_min) / (array_max - array_min)\n",
    "    clip[clip > 1] = 1\n",
    "    clip[clip < 0] = 0\n",
    "    return clip\n",
    "\n",
    "def false_colour(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        h,w = src.shape\n",
    "        img = np.zeros((3,h,w))\n",
    "        print(img.shape)\n",
    "        for i in range(3):\n",
    "            img[i,:,:]= pct_clip(src.read(i+1))\n",
    "            \n",
    "    return img, src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Parameters**:\n",
    "  - `data`: Data file holding image paths.\n",
    "  - `name` (str): Path to save images.\n",
    "- **Process**:\n",
    "  - Reads and processes each image and mask.\n",
    "  - Displays images and masks in a figure.\n",
    "  - Saves the figure to the specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(data, name, visualization_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        save all images into single figure\n",
    "    Arguments:\n",
    "        data : data file holding images path\n",
    "        directory (str) : path to save images\n",
    "    Return:\n",
    "        save images figure into directory\n",
    "    \"\"\"\n",
    "    \n",
    "    pathlib.Path(visualization_dir / 'display').mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(visualization_dir / \"display\"/\"train\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(visualization_dir / \"display\"/\"test\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(visualization_dir / \"display\"/\"valid\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        image,src = false_colour(data.feature_ids.values[i])\n",
    "        print(\"................................\")\n",
    "        print(np.mean(image),np.std(image))\n",
    "        print(\"................................\")\n",
    "        mask = read_img(data.masks.values[i], label=True)\n",
    "        print(\"................................\")\n",
    "        print(f\"image_shape: {image.shape}\")\n",
    "        print(f\"mask_shape: {mask.shape}\")\n",
    "        print(\"................................\")\n",
    "        id = data.feature_ids.values[i].split(\"/\")[-1]\n",
    "        display_list = {\"image\": image, \"label\": mask}\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        title = list(display_list.keys())\n",
    "\n",
    "        for i in range(len(display_list)):\n",
    "            plt.subplot(1, len(display_list), i + 1)\n",
    "            plt.title(title[i])\n",
    "            if title[i]=='image':\n",
    "                ax = plt.gca()\n",
    "                show(display_list[title[i]],transform=src.transform, ax=ax)\n",
    "            else:\n",
    "                plt.imshow((display_list[title[i]]), cmap=\"gray\") # for mask without boundary\n",
    "                # plt.imshow((display_list[title[i]]))  # for mask with boundary\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        img_name = \"{}.png\".format(id)  # create file name to save\n",
    "        plt.savefig(\n",
    "            os.path.join((visualization_dir / \"display\"/ name), img_name),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=800,\n",
    "        )\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays and saves training images and masks using the `display_all` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_dir = pathlib.Path(f\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/signal_based_data/{dir_name}\")\n",
    "\n",
    "print(f\"displaying training images and masks for {dir_name} \\n\")\n",
    "\n",
    "# display_all(data = train_df,\n",
    "#             name = \"train\",\n",
    "#             visualization_dir = visualization_dir)\n",
    "\n",
    "# display_all(data = valid_df,\n",
    "#             name = \"valid\",\n",
    "#             visualization_dir = visualization_dir)\n",
    "\n",
    "display_all(data = test_df,\n",
    "            name = \"test\",\n",
    "            visualization_dir = visualization_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculating Mean & Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads training dataset CSV and defines a function to calculate mean and standard deviation for each band of the images.\n",
    "\n",
    "### Actions:\n",
    "- Loads training dataset CSV.\n",
    "- Defines `calculate_stats` to:\n",
    "  - Read and clip the first three bands of each image.\n",
    "  - Calculate and print the mean and standard deviation for each band.\n",
    "- Calls `calculate_stats` with the list of feature image paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = train_df[\"feature_ids\"].to_list()\n",
    "\n",
    "def pct_clip(array, pct=[2.5, 97.5]):\n",
    "    array_min, array_max = np.nanpercentile(array, pct[0]), np.nanpercentile(array, pct[1])\n",
    "    clip = (array - array_min) / (array_max - array_min)\n",
    "    clip[clip > 1] = 1\n",
    "    clip[clip < 0] = 0\n",
    "    return clip\n",
    "\n",
    "def calculate_stats(file_paths):\n",
    "    all_data1 = []\n",
    "    all_data2 = []\n",
    "    all_data3 = []\n",
    "    for file_path in file_paths:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data1 = pct_clip(src.read(1))  # Read the first band\n",
    "            all_data1.append(data1)\n",
    "            data2 = pct_clip(src.read(2))  # Read the second band\n",
    "            all_data2.append(data2)\n",
    "            data3 = pct_clip(src.read(3))  # Read the third band\n",
    "            all_data3.append(data3)\n",
    "\n",
    "    # Stack all the data into a single numpy array\n",
    "    stacked_data1 = np.stack(all_data1)\n",
    "    stacked_data2 = np.stack(all_data2)\n",
    "    stacked_data3 = np.stack(all_data3)\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean1 = np.mean(stacked_data1)\n",
    "    std_dev1 = np.std(stacked_data1)\n",
    "    mean2 = np.mean(stacked_data2)\n",
    "    std_dev2 = np.std(stacked_data2)\n",
    "    mean3 = np.mean(stacked_data3)\n",
    "    std_dev3 = np.std(stacked_data3)\n",
    "    \n",
    "    return mean1, mean2, mean3, std_dev1, std_dev2, std_dev3\n",
    "    \n",
    "def calculate_average(file_paths):\n",
    "    all_data = []\n",
    "    for file_path in file_paths:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data = pct_clip(src.read()) \n",
    "            # print(data.shape)\n",
    "            all_data.append(data)\n",
    "\n",
    "    # Stack all the data into a single numpy array\n",
    "    stacked_data = np.stack(all_data)\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean = np.mean(stacked_data)\n",
    "    std_dev = np.std(stacked_data)\n",
    "\n",
    "    return mean, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1, mean2, mean3, std_dev1, std_dev2, std_dev3 = calculate_stats(features_path)\n",
    "mean, std_dev = calculate_average(features_path)\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Average mean across all files:\", mean)\n",
    "print(\"Standard deviation across all files:\", std_dev)\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"1st band (mean):\", mean1)\n",
    "print(\"1st band (std):\", std_dev1)\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"2nd band (mean):\", mean2)\n",
    "print(\"2nd band (std):\", std_dev2)\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"3rd band (mean):\", mean3)\n",
    "print(\"3rd band (std):\", std_dev3)\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tiles Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines `save_tiles` to split large images into smaller tiles and save them.\n",
    "\n",
    "### Function: `save_tiles(path, out_path, tiles_size=2048, stride=1024)`\n",
    "- **Parameters**:\n",
    "  - `path`: Directory with original images.\n",
    "  - `out_path`: Directory to save the tiles.\n",
    "  - `tiles_size`: Size of each tile.\n",
    "  - `stride`: Stride for tiling.\n",
    "- **Process**: Iterates through images, splits them into tiles, and saves the tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tiles(path, out_path, tiles_size, stride):\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each file in the path\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Get metadata and calculate number of tiles in each dimension\n",
    "            meta = src.meta\n",
    "            meta[\"height\"]= tiles_size\n",
    "            meta[\"width\"]= tiles_size\n",
    "            # print(meta)\n",
    "            height, width = src.shape\n",
    "            num_rows = math.ceil((height - tiles_size) / stride + 1)\n",
    "            num_cols = math.ceil((width - tiles_size) / stride + 1)\n",
    "            total_tiles = num_rows* num_cols\n",
    "            print(f\"shape of the image before tiles : {src.shape}\")\n",
    "            print(f\"number of tiles={total_tiles}\")\n",
    "            print(\"..................................................\")\n",
    "            # Iterate over each tile\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    # Calculate window coordinates\n",
    "                    row_start = row * stride\n",
    "                    row_stop = min(row_start + tiles_size, height)\n",
    "                    col_start = col * stride\n",
    "                    col_stop = min(col_start + tiles_size, width)\n",
    "                    \n",
    "                    # Read the tile data\n",
    "                    # window = Window(x0, y0, x1 - x0, y1 - y0)\n",
    "                    window = Window.from_slices((row_stop-stride, row_stop), (col_stop-stride, col_stop))\n",
    "                    tile_data = src.read(window=window)\n",
    "                    # print(\"...........\")\n",
    "                    # print(tile_data.shape)\n",
    "                    # Save the tile with a suffix of tile id\n",
    "                    out_filename = f\"tile_{row}_{col}_{os.path.splitext(filename)[0]}.tif\"\n",
    "                    out_file_path = os.path.join(out_path, out_filename)\n",
    "                    with rasterio.open(out_file_path, 'w', **meta) as dst:\n",
    "                        dst.write(tile_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/signal_based_data/{dir_name}-full/groundtruth\"\n",
    "out_path = f\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/signal_based_data/{dir_name}/groundtruth\"\n",
    "\n",
    "# path = \"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/signal_based_data/groundtruth/gt_full\"\n",
    "# out_path = \"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/signal_based_data/groundtruth/gt_2048_1024\"\n",
    "\n",
    "print(path)\n",
    "print(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tiles(path, out_path, tiles_size = 2048, stride = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(datapath):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(datapath)\n",
    "    \n",
    "    for filename in files:\n",
    "        # Extract the file extension\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        \n",
    "        # Check if the filename starts with DEM_ab.tif\n",
    "        if filename.startswith(\"DEM_\"):\n",
    "            new_filename = filename.replace(\"DEM_\", \"\").replace(\".tif\", \"_nasadem.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VV_ab.tif\n",
    "        elif filename.startswith(\"VV_\"):\n",
    "            new_filename = filename.replace(\"VV_\", \"\").replace(\".tif\", \"_vv.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VH_ab.tif\n",
    "        elif filename.startswith(\"VH_\"):\n",
    "            new_filename = filename.replace(\"VH_\", \"\").replace(\".tif\", \"_vh.tif\")\n",
    "        \n",
    "        # Check if the filename starts with GT_ab.tif\n",
    "        elif filename.startswith(\"GT_\"):\n",
    "            new_filename = filename.replace(\"GT_\", \"\")\n",
    "        \n",
    "        else:\n",
    "            # If none of the conditions are met, skip this file\n",
    "            raise ValueError(\"files_name_mismatch\")\n",
    "        \n",
    "        # Construct the new filepath\n",
    "        new_filepath = os.path.join(datapath, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(os.path.join(datapath, filename), new_filepath)\n",
    "        print(f\"Renamed {filename} to {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls the `rename_files` function to rename files in the specified dataset directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = config.dataset_dir\n",
    "rename_files(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save all Output in Visualization in rtf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs a visualization script and saves the terminal output to an RTF file.\n",
    "\n",
    "### Actions:\n",
    "- Executes the `visualization.py` script using a terminal command.\n",
    "- Captures the terminal output.\n",
    "- Saves the output to `data_statistics.rtf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command in the terminal\n",
    "command = \"python visualization.ipynb\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Get the terminal output\n",
    "terminal_output = result.stdout\n",
    "\n",
    "# Save the output to an RTF file\n",
    "rtf_filename = \"data_statistics.rtf\"\n",
    "with open(rtf_filename, \"w\") as rtf_file:\n",
    "    # rtf_file.write(\"{\\\\rtf1\\\\ansi\\n\")\n",
    "    rtf_file.write(terminal_output)\n",
    "    # rtf_file.write(\"}\")\n",
    "\n",
    "print(f\"Terminal output saved to {rtf_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
